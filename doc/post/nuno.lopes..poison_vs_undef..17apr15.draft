To:  TBD
Tentative-To: John Regehr <regehr@cs.utah.edu> 
Subject: Re:  Fwd: Fixing LLVM's IR

Nuno, David:

I am one of John's graduate students who has been working on this
problem.  In particular, I have created a LLVM IR interpreter that
tracks poison, and is capable of applying different poison handling
rules and reporting the effects.  The proposal in your 16apr2015 email
brings up some interesting ideas, and I believe John's and my findings
can help refine them.

 > Inconsistencies:
 > Right now, LLVM is inconsistent regarding the set of allowed
 > transformations. Consider the following:
 > %v = select %c, %a, undef
 > 
 > LLVM will transform this into:
 > %v = %a
 > 
 > However, if %a is poison, and %c=true, this transformation is not really
 > valid. Before we had an arbitrary bit pattern (undef), and now we have a
 > poison value, which cannot be untainted.

First of all, my understanding of the undef value is that the compiler
can assume it is anything at all.  I have always pictured "anything"
as including a poison value.  Therefore the above transformation is
fully valid.  I picture this transformation as also poisoning %v.
Does this make sense to you, or do you see this leading to messy
consequences?

Second, I have been moving towards the conclusion that the spec should
adopt David's earlier suggestion of untainting ("antidoting") poison
values by declaring that if the result of an operation can be
determined without using a poison operand, then the result is
unpoisoned.  I think of this as the "antidote if poison operand
unused" rule.  (After reading your email, I wonder if this concept
could be applied to your proposed merger of poison and undef, but more
on that below.)

To give an example, consider:

	%icmp50= icmp sgt %arg1, %int_min
	br %icmp50 label %a, label %b
a:
	%51= sub nsw %arg1, 1  ; produces poison iff %arg1==%int_min
	%br label %c
b:
	%52= add %arg1, %arg2
c:
	%53= phi [ label a, %51 ], [ label b, %52 ]

The "Simplify the Control Flow Graph" optimization pass reduces this to:

	%51= sub nsw %arg1, 1  ; produces poison iff %arg1==%int_min
	%52= add %arg1, %arg2
	%icmp50= icmp sgt %arg1, %int_min
	%53= select %icmp50, %51, %52

Under the current poison handling rules, if %arg1==%int_min, then %53
is unpoisoned in the original code, but is poisoned in the optimized
code.  But if we apply David's "antidote if poison operand unused"
rule, then %53 is fine.

There are more opportunities for the optimizer to introduce poison
when there are multiple icmp instructions.  Consider this example,
which is the LLVM representation of an actual function from a C program:

define i32 @x3(i64 %x4) #0 {
  %1 = load i64* @x0, align 8
  %2 = icmp eq i64 %1, 0
  br i1 %2, label %6, label %3

; <label>:3                                       ; preds = %0
  %4 = sub nsw i64 9223372036854775807, %x4
  %5 = icmp sgt i64 %1, %4
  br i1 %5, label %7, label %6

; <label>:6                                       ; preds = %0, %3
  br label %7

; <label>:7                                       ; preds = %6, %3
  %8 = phi i32 [ 1, %3 ], [ 0, %6 ]
  ret i32 %8
}

The "Simplify the Control Flow Graph" optimization pass reduced this to:

define i32 @x3(i64 %x4) #0 {
  %1 = load i64* @x0, align 8
  %2 = icmp ne i64 %1, 0
  %3 = sub nsw i64 9223372036854775807, %x4
  %4 = icmp sgt i64 %1, %3
  %or.cond = and i1 %2, %4
  %5 = select i1 %or.cond, i32 1, i32 0
  ret i32 %5
}

Assuming that the subtraction generates poison in both the raw and
optimized cases, the raw case only returns with poison if %1 != 0,
while the optimized case ends with poison unconditionally.  However,
applying David's "antidote if poison operand unused" rule at the and
instruction would make %or.cond (and the return value) only be
poisoned if %1 != 0, which is the same behavior as the raw case.

When I mentally run these cases following the proposal that unwanted
overflow produces the undef value, I get the same results that the
"antidote if poison operand unused" rule produces.  (Do you consur?)
This observation has led to the following thoughts:

I agree that the combination of undefined behavior, undef
value, poison, and normal values is rather complicated.  So maybe
David's and Nuno's poison==undef proposal can help the LLVM community find a
simpler, more general solution than a mere tweak of the poison rules.

That said, I keep coming back to this thought: with some kind of a
poison flag accompanying a value, the compiler can do a simplistic
static analysis and warn a user if the result of a wraparound is
likely to affect an output value.  Without a poison concept, it can't.
Maybe this is ok-- we might decide that such a warning is the
responsibility of a separate static analyzer, not of the compiler.  If
so, we should have some mechanism to tell the separate static analyzer
what the optimized code looks like, so the compiler optimizations
don't invalidate the static analysis.  (Footnote: yes, in an ideal
world, the source language specification would only have one
interpretation.  But that isn't necessarily true in reality, leading
to the risk that the compiler optimizer and the static analyzer might
have slightly different interpretations of the spec. Hence the
importance of telling the static analyzer what the compiler was
thinking when it ran its optimization pass.)

 > Now, how do we know this is what we want?  There are two angles to answer
 > this questions:
 >   - Correctness: is this IR semantics consistent?
 >   - Performance: can we do all (most?) the transformations we care about?
 > 
 > Regarding correctness, we think the proof we need to do is the following:
 > Given a well-defined program, apply any set of valid transformations. The
 > resulting program is well-defined.
 > 
 > The proof for this kind of theorem is complicated. It is done by structural
 > induction over the syntax of programs, which is non-trivial to automate.
 > I suggested we could start by taking all programs up to, say, 5
 > instructions, and prove that any set of transformations still generates a
 > well-defined program.  

The fuzzer I'm working on might help with this.  

Further reflection: This might be doable by a proof by exhaustion.  I count
56 relevant llvm instructions, of which about half are pretty rarely used in
csmith programs (like insertelement).  28^5= 17e6 cases, which is doable
with some parallelism.  A full treatment (56^5) is 550e6 cases, is still
doable with more parallelism.

 > When we get some confidence, then do the proof for
 > unbounded program lengths.

Of course, I reserve the result to change my mind as I think about this
further.  :-)

Best wishes,

-- Christian

-----------------------------------------------------------------------------
Christian Schreiner
cas@cs.utah.edu

